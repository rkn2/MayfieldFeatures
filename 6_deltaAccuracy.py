import pandas as pd
import numpy as np
import os
import joblib
import time
import warnings
import matplotlib.pyplot as plt
import seaborn as sns
import re
import ast
import logging
import sys
import json

from sklearn.metrics import make_scorer, f1_score
from sklearn.inspection import permutation_importance
from dython.nominal import associations
from scipy.cluster import hierarchy
from scipy.spatial.distance import squareform
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, HistGradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier

try:
    import xgboost as xgb

    XGB_AVAILABLE = True
except ImportError:
    XGB_AVAILABLE = False
try:
    import lightgbm as lgb

    LGBM_AVAILABLE = True
except ImportError:
    LGBM_AVAILABLE = False
try:
    import mord
    MORD_AVAILABLE = True
except ImportError:
    MORD_AVAILABLE = False


# --- Logging Configuration Setup ---
def setup_logging(log_file='pipeline.log'):
    """Sets up logging to both a file and the console."""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler(log_file, mode='a'),
            logging.StreamHandler(sys.stdout)
        ],
        force=True
    )


setup_logging()

# --- Configuration ---
DATA_DIR = 'processed_ml_data'
RESULTS_DIR_4 = 'clustering_performance_results'
BASE_RESULTS_DIR = 'cluster_importance_results_final'

# --- Paths ---
TRAIN_X_PATH = os.path.join(DATA_DIR, 'X_train_processed.pkl')
TRAIN_Y_PATH = os.path.join(DATA_DIR, 'y_train.pkl')
TEST_X_PATH = os.path.join(DATA_DIR, 'X_test_processed.pkl')
TEST_Y_PATH = os.path.join(DATA_DIR, 'y_test.pkl')
# *** FIXED ***: This now points to the correct file name generated by script 4.
DETAILED_PERFORMANCE_CSV = os.path.join(RESULTS_DIR_4,
                                        'clustering_performance_detailed_results.csv')

# --- Analysis Settings ---
CLUSTERING_LINKAGE_METHOD = 'average'
SCORING_METRIC_FOR_IMPORTANCE = 'f1_weighted'
PERFORMANCE_THRESHOLD = 0.85
P_VALUE_THRESHOLD = 0.05
RANDOM_STATE = 42
N_PERMUTATION_REPEATS = 100



# --- Helper Functions ---
def load_data(file_path, description="data"):
    logging.info(f"Loading {description} from {file_path}...")
    try:
        data = joblib.load(file_path)
        logging.info(f"  Successfully loaded: {description}")
        return data
    except FileNotFoundError:
        logging.error(f"Error: {description} file not found at {file_path}. Exiting.")
        sys.exit(1)
    except Exception as e:
        logging.error(f"Error loading {description} from {file_path}: {e}", exc_info=True)
        sys.exit(1)


def sanitize_feature_names_df(df):
    if not isinstance(df, pd.DataFrame): return df
    new_cols = [re.sub(r'[^A-Za-z0-9_]+', '_', col) for col in df.columns]
    new_cols = [re.sub(r'[\[\]<]', '_', col) for col in new_cols]
    df.columns = new_cols
    return df


def get_selected_features_by_clustering(original_df, distance_thresh, linkage_meth):
    if distance_thresh is None or pd.isna(distance_thresh):
        logging.info("  Threshold is None. Using all original features.")
        return original_df.columns.tolist()

    feature_names_list = original_df.columns.tolist()
    if len(feature_names_list) <= 1: return feature_names_list

    logging.info("  Calculating association matrix for clustering...")
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        assoc_df = associations(original_df, nom_nom_assoc='cramer', compute_only=True)['corr'].fillna(0)

    logging.info("  Performing hierarchical clustering...")
    distance_mat = 1 - np.abs(assoc_df.values)
    np.fill_diagonal(distance_mat, 0)
    condensed_dist_mat = squareform(distance_mat, checks=False)
    if condensed_dist_mat.shape[0] == 0: return feature_names_list

    linked = hierarchy.linkage(condensed_dist_mat, method=linkage_meth)
    cluster_labels_arr = hierarchy.fcluster(linked, t=distance_thresh, criterion='distance')

    selected_representatives_list = []
    for i in range(1, len(np.unique(cluster_labels_arr)) + 1):
        cluster_indices = [idx for idx, label in enumerate(cluster_labels_arr) if label == i]
        if not cluster_indices: continue

        if len(cluster_indices) == 1:
            selected_representatives_list.append(feature_names_list[cluster_indices[0]])
        else:
            sum_abs_assoc = np.abs(assoc_df.iloc[cluster_indices, cluster_indices].values).sum(axis=1)
            rep_local_idx = np.argmax(sum_abs_assoc)
            selected_representatives_list.append(feature_names_list[cluster_indices[rep_local_idx]])

    return sorted(list(set(selected_representatives_list)))


# --- Main Orchestration ---
def main():
    logging.info(f"--- Starting Script: 6_deltaAccuracy.py (F1 > {PERFORMANCE_THRESHOLD} with P-Value) ---")
    warnings.filterwarnings("ignore", category=UserWarning)

    # Step 1: Identify high-performing model-feature set combinations
    logging.info(
        f"Identifying combinations with Test F1 Weighted > {PERFORMANCE_THRESHOLD} from '{DETAILED_PERFORMANCE_CSV}'...")
    try:
        performance_df = pd.read_csv(DETAILED_PERFORMANCE_CSV)
        high_performing_combinations = performance_df[
            performance_df['Test F1 Weighted'] > PERFORMANCE_THRESHOLD].sort_values(by='Test F1 Weighted',
                                                                                    ascending=False)

        if high_performing_combinations.empty:
            logging.warning(
                f"No model combinations found with a Test F1 Weighted score > {PERFORMANCE_THRESHOLD}. Exiting.")
            sys.exit(0)

        logging.info(f"{len(high_performing_combinations)} high-performing combinations identified:")
        logging.info(high_performing_combinations[['Model', 'Feature Set Name', 'Test F1 Weighted']].to_string())
    except FileNotFoundError:
        logging.error(f"FATAL: Detailed performance file not found at '{DETAILED_PERFORMANCE_CSV}'.")
        sys.exit(1)
    except Exception as e:
        logging.error(f"FATAL: Could not read or process performance file. Error: {e}", exc_info=True)
        sys.exit(1)

    # Step 2: Set up models and load data
    all_models = {
        "LightGBM": lgb.LGBMClassifier(random_state=RANDOM_STATE, verbosity=-1) if LGBM_AVAILABLE else None,
        "Gradient Boosting": GradientBoostingClassifier(random_state=RANDOM_STATE),
        "XGBoost": xgb.XGBClassifier(random_state=RANDOM_STATE, eval_metric='mlogloss') if XGB_AVAILABLE else None,
        "Hist Gradient Boosting": HistGradientBoostingClassifier(random_state=RANDOM_STATE),
        #"KNN": KNeighborsClassifier(), # overly sensitive to parameter of inputs and i have too many inputs
        "Logistic Regression": LogisticRegression(random_state=RANDOM_STATE, max_iter=2000, solver='liblinear'),
        "Random Forest": RandomForestClassifier(random_state=RANDOM_STATE),
        "Decision Tree": DecisionTreeClassifier(random_state=RANDOM_STATE),
        "Ordinal LAD": mord.LAD() if MORD_AVAILABLE else None,
        "Ordinal Ridge": mord.OrdinalRidge() if MORD_AVAILABLE else None,
        "Ordinal Logistic (AT)": mord.LogisticAT() if MORD_AVAILABLE else None,
    }

    X_train_orig = load_data(TRAIN_X_PATH, "original X_train")
    y_train = load_data(TRAIN_Y_PATH, "original y_train")
    X_test_orig = load_data(TEST_X_PATH, "original X_test")
    y_test = load_data(TEST_Y_PATH, "original y_test")

    X_train_sanitized = sanitize_feature_names_df(pd.DataFrame(X_train_orig))
    X_test_sanitized = sanitize_feature_names_df(pd.DataFrame(X_test_orig)).reindex(columns=X_train_sanitized.columns,
                                                                                    fill_value=0)
    y_train_ravel = y_train.ravel()
    y_test_ravel = y_test.ravel()

    # Step 3: Loop through each top combination and run analysis
    all_model_importances = []

    for rank, (index, combo_row) in enumerate(high_performing_combinations.iterrows(), 1):
        model_name = combo_row['Model']
        threshold = combo_row['Threshold Value']
        params_str = combo_row['Best Params']

        logging.info(f"\n===== Analyzing Combination {rank}/{len(high_performing_combinations)} =====")
        logging.info(f"Model: {model_name}, Threshold: {threshold}")

        selected_features = get_selected_features_by_clustering(X_train_sanitized, threshold, CLUSTERING_LINKAGE_METHOD)
        if not selected_features:
            logging.warning(
                f"  Skipping combination for {model_name} at threshold {threshold} as no features were selected.")
            continue
        logging.info(f"  Number of features for this combination: {len(selected_features)}")

        X_train_selected = X_train_sanitized[selected_features]
        X_test_selected = X_test_sanitized[selected_features]

        model_template = all_models.get(model_name)
        if model_template is None:
            logging.warning(f"  Skipping model '{model_name}' as it's not available in this environment.")
            continue

        try:
            best_params = ast.literal_eval(params_str) if isinstance(params_str, str) and params_str != 'nan' else {}
        except Exception:
            logging.warning(f"  Could not parse params for {model_name}: '{params_str}'. Using defaults.")
            best_params = {}

        model_instance = model_template.set_params(**best_params)

        logging.info(f"  Retraining {model_name} on {len(selected_features)} features...")
        model_instance.fit(X_train_selected, y_train_ravel)

        logging.info(f"  Calculating Permutation Importance...")
        scorer = make_scorer(f1_score, average='weighted', zero_division=0)
        perm_importance_result = permutation_importance(
            model_instance, X_test_selected, y_test_ravel, scoring=scorer,
            n_repeats=N_PERMUTATION_REPEATS, random_state=RANDOM_STATE, n_jobs=-1
        )

        for i, feature_name in enumerate(selected_features):
            count_le_zero = np.sum(perm_importance_result.importances[i] <= 0)
            p_value = (count_le_zero + 1) / (N_PERMUTATION_REPEATS + 1)

            all_model_importances.append({
                'Cluster Label': feature_name,
                'Model (Thresh)': f"{model_name} ({'Orig' if pd.isna(threshold) else threshold})",
                'Importance (Mean Drop)': perm_importance_result.importances_mean[i],
                'p-value': p_value
            })

    # Step 4: Visualize the results
    if not all_model_importances:
        logging.error("No importance results were generated. Cannot create plot.")
        sys.exit(1)

    importances_df = pd.DataFrame(all_model_importances)

    significant_df = importances_df[importances_df['p-value'] < P_VALUE_THRESHOLD].copy()

    if significant_df.empty:
        logging.warning(f"No statistically significant feature clusters found with p-value < {P_VALUE_THRESHOLD}.")
        logging.warning(
            "Plot will be empty. Consider increasing N_PERMUTATION_REPEATS for more stable p-values or adjusting the threshold.")
    else:
        num_combos = len(high_performing_combinations)
        logging.info(
            f"Found {len(significant_df['Cluster Label'].unique())} statistically significant feature clusters to plot.")

        max_importance_order = significant_df.groupby('Cluster Label')['Importance (Mean Drop)'].max().sort_values(
            ascending=False).index

        logging.info(f"\n--- Data for Final Permutation Importance Bar Chart ---")
        plot_df_to_log = significant_df[significant_df['Cluster Label'].isin(max_importance_order)].copy()
        plot_df_to_log['Cluster Label'] = pd.Categorical(plot_df_to_log['Cluster Label'], categories=max_importance_order, ordered=True)
        logging.info(f"\n{plot_df_to_log.sort_values(by=['Cluster Label', 'Importance (Mean Drop)']).to_string()}")

        plt.figure(figsize=(16, max(8, len(max_importance_order) * 0.5)))
        sns.barplot(x='Importance (Mean Drop)', y='Cluster Label', hue='Model (Thresh)', data=significant_df,
                    palette='viridis',
                    order=max_importance_order)

        plot_title = f'Statistically Significant Feature Clusters (p < {P_VALUE_THRESHOLD}) for Top {num_combos} Model Combinations (F1 > {PERFORMANCE_THRESHOLD})'
        plt.title(plot_title, fontsize=16, pad=20)
        plt.xlabel(f"Mean Drop in Test {SCORING_METRIC_FOR_IMPORTANCE.replace('_', ' ').title()}", fontsize=12)
        plt.ylabel("Feature Cluster Representative", fontsize=12)
        plt.legend(title='Model (Threshold)')
        plt.tight_layout()

        results_subdir = os.path.join(BASE_RESULTS_DIR, f"f1_gt_{str(PERFORMANCE_THRESHOLD).replace('.', '')}_combos")
        os.makedirs(results_subdir, exist_ok=True)

        plot_save_path = os.path.join(results_subdir, "significant_cluster_importances.png")
        plt.savefig(plot_save_path, bbox_inches='tight')
        logging.info(f"\nFinal importance plot saved to: {plot_save_path}")
        plt.show()

    logging.info("\n--- Full Cluster Importance Results (Top Performing Combinations) ---")
    with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.width', 1000):
        logging.info(
            f"\n{importances_df.sort_values(by=['Model (Thresh)', 'Importance (Mean Drop)'], ascending=[True, False]).to_string()}")

    logging.info(f"--- Finished Script: 6_deltaAccuracy.py (F1 > {PERFORMANCE_THRESHOLD} with P-Value) ---")


if __name__ == '__main__':
    main()
